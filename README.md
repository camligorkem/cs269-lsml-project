# cs269-lsml-project

[Original LTH code reference](https://github.com/rahulvigneswaran/Lottery-Ticket-Hypothesis-in-Pytorch)
[Adversary examples created using DeepRobust library](https://github.com/DSE-MSU/DeepRobust)

### Create Environment

Run the following commands to create the environment:
```
conda create --name lsml python=3.6

conda activate lsml

pip install -r requirements2.txt

```

Run the LTH code:

```
python3 main.py --prune_type=lt --arch_type=fc1 --dataset=mnist_fgsm_attack --prune_percent=90 --prune_iterations=2 --end_iter=3

```

You can modify the parameters based on  [Original LTH code reference](https://github.com/rahulvigneswaran/Lottery-Ticket-Hypothesis-in-Pytorch#readme)

__Global numbers to run on experiments:__

--prune_percent=10     
--prune_iterations=35   

We should set --end_iter is the number of epochs trained in each pruning iteration. We should pick it
based on our dataset since MNIST might be more quickly overfit than bigger/complex datasets. Don't stick with default since it is 100 and very big for small datasets.
For MNIST, we can make it around 12.

Also make sure to run same experiment with both --prune_type=lt and --prune_type=reinit.


### Research Questions:
1. Can we find winning tickets with attacked dataset?
2. Are winning tickets same for org. & attacked dataset? If diff by how much? (Experiment 3)
3. How attack rate on dataset changes LTH results? (Experiment 1)
4. Different attacks have diff. effects on LTH results? ( Experiment 2)
5. Different type of architectures on same attacked dataset affected differently from the attacks or have same LTH results/accuracy? (Experiment 4)
6. Can we see similar results on different Datasets (CIFAR10) - (Experiment 5)


### Experiments :

1. Use Original + Generated Dataset (Compare performance for dataset ratios)
 a. 50% original MNIST + 50 % adversarial samples

2. Check different attack types and how they are behaving in terms of LTH:
example attacks (2-3) [FGSM, PGD, CW]

3. Compare winning tickets generated by original vs generated dataset
write code to read pruned networks and compare against each other, check how much they are similar? ie. also define similarity metric.

4. Different architectures [VGG,ResNet..]

5. Different datasets (CIFAR10)
6. Oneshot pruning vs iterative pruning experiment.
